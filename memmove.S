/* Runtime ABI for the ARM Cortex-M0  
 * memmove.S: move memory block
 *
 * Copyright (c) 2017 JÃ¶rg Mische <bobbl@gmx.de>
 *
 * Permission to use, copy, modify, and/or distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
 * OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */



	.syntax unified
	.text
	.thumb
	.cpu cortex-m0



@ void __aeabi_memmove8(void *r0, const void *r1, size_t r2);
@
@ Move r2 bytes from r1 to r0 and check for overlap.
@ r1 and r0 must be aligned to 8 bytes.
@
	.thumb_func
	.global __aeabi_memmove8
__aeabi_memmove8:



@ void __aeabi_memmove4(void *r0, const void *r1, size_t r2);
@
@ Move r2 bytes from r1 to r0 and check for overlap.
@ r1 and r0 must be aligned to 4 bytes.
@
	.thumb_func
	.global __aeabi_memmove4
__aeabi_memmove4:



	cmp	r0, r1
	bls	__aeabi_memcpy4
	adds	r3, r1, r2
	cmp	r0, r3
	bhs	__aeabi_memcpy4

	b	.Lbackward_entry

.Lbackward_loop:
	ldrb	r3, [r1, r2]
	strb	r3, [r0, r2]

.Lbackward_entry:
	subs	r2, #1
	bhs	.Lbackward_loop

	bx	lr



@ void __aeabi_memmove(void *r0, const void *r1, size_t r2);
@
@ Move r2 bytes from r1 to r0 and check for overlap.
@ r0 and r1 need not be aligned.
@
	.thumb_func
	.global __aeabi_memmove
__aeabi_memmove:



	cmp	r0, r1
	bls	__aeabi_memcpy
	adds	r3, r1, r2
	cmp	r0, r3
	blo	.Lbackward_entry



@ void __aeabi_memcpy(void *r0, const void *r1, size_t r2);
@
@ Move r2 bytes from r1 to r0. No overlap allowed.
@ r0 and r1 need not be aligned.
@
	.thumb_func
	.global __aeabi_memcpy
__aeabi_memcpy:



	cmp	r2, #8
	blo	.Lforward1
	mov	r3, r0
	eors	r3, r1
	lsls	r3, r3, #30
	bne	.Lforward1

	lsrs	r3, r0, #1
	bcc	.Lalign2
	ldrb	r3, [r1]
	strb	r3, [r0]
	adds	r0, #1
	adds	r1, #1
	subs	r2, #1
.Lalign2:
	lsrs	r3, r0, #2
	bcc	.Lalign4
	ldrh	r3, [r1]
	strh	r3, [r0]
	adds	r0, #2
	adds	r1, #2
	subs	r2, #2
.Lalign4:



@ void __aeabi_memcpy8(void *r0, const void *r1, size_t r2);
@
@ Move r2 bytes from r1 to r0. No overlap allowed.
@ r0 and r1 must be aligned to 8 bytes.
@
	.thumb_func
	.global __aeabi_memcpy8
__aeabi_memcpy8:



@ void __aeabi_memcpy4(void *r0, const void *r1, size_t r2);
@
@ Move r2 bytes from r1 to r0. No overlap allowed.
@ r0 and r1 must be aligned to 4 bytes.
@
	.thumb_func
	.global __aeabi_memcpy4
__aeabi_memcpy4:



	subs	r2, #20
	blo	.Lforward4
	push	{r4, r5, r6, r7}
.Lforward20_loop:
	ldm	r1!, {r3, r4, r5, r6, r7}
	stm	r0!, {r3, r4, r5, r6, r7}
	subs	r2, #20
	bhs	.Lforward20_loop
	pop	{r4, r5, r6, r7}

.Lforward4:
	adds	r2, #16
	blo	.Lforward4_corr
.Lforward4_loop:
	ldm	r1!, {r3}
	stm	r0!, {r3}
	subs	r2, #4
	bhs	.Lforward4_loop

.Lforward4_corr:
	adds	r2, #4

.Lforward1:
	orrs	r2, r2
	beq	9f
	push	{r4}
	eors	r4, r4

.Lforward1_loop:
	ldrb	r3, [r1, r4]
	strb	r3, [r0, r4]
	adds	r4, #1
	cmp	r4, r2
	blo	.Lforward1_loop
	pop	{r4}
9:	bx	lr
